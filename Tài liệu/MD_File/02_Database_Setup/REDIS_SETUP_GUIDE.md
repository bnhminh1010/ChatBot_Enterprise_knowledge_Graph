# ðŸš€ Quick Setup Guide - Redis + Improvements

## âœ… Nhá»¯ng gÃ¬ Ä‘Ã£ hoÃ n thÃ nh

### 1. **Phase 1: Improved QueryClassifier**

- âœ… Confidence scoring system
- âœ… Expanded patterns for medium queries
- âœ… Stricter complex query detection
- File: `src/ai/query-classifier.service.ts`

### 2. **Redis Conversation Storage** (thay Neo4j)

- âœ… RedisConversationService created
- âœ… In-memory, fast (< 1ms)
- âœ… Auto TTL cleanup (7 days)
- âœ… Docker setup ready
- File: `src/chat/services/redis-conversation.service.ts`

### 3. **Ollama RAG for Medium Queries** (giáº£m Gemini usage)

- âœ… OllamaRAGService created
- âœ… Local LLM (FREE, no API cost)
- âœ… ChromaDB retrieval + Ollama generation
- File: `src/chat/services/ollama-rag.service.ts`

### 4. **ChatService Updated**

- âœ… Redis conversation tracking
- âœ… Ollama RAG for semantic searches
- âœ… Fallback chain: Ollama â†’ ChromaDB â†’ Text search

---

## ðŸ³ Setup Redis Docker

### Option A: Merge vÃ o docker-compose.yml hiá»‡n cÃ³

```yaml
# Add vÃ o docker-compose.yml
services:
  # ... existing services ...

  redis:
    image: redis:7-alpine
    container_name: ekg-redis
    ports:
      - '6379:6379'
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    restart: unless-stopped

volumes:
  # ... existing volumes ...
  redis-data:
```

### Option B: Cháº¡y riÃªng

```bash
docker-compose -f docker-compose.redis.yml up -d
```

### Verify Redis running

```bash
docker ps | grep redis
# Should see: ekg-redis running

docker exec -it ekg-redis redis-cli ping
# Should return: PONG
```

---

## ðŸ“ Environment Variables

Add to `.env`:

```bash
# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
# REDIS_PASSWORD=  # Optional
```

---

## ðŸŽ¯ How it Works Now

### Query Flow

```
User Query
    â†“
QueryClassifier (improved vá»›i confidence scoring)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Simple    â”‚   Medium    â”‚   Complex   â”‚
â”‚   (30%)     â”‚   (50%)     â”‚   (20%)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“             â†“             â†“
   Neo4j      Ollama RAG      Gemini
   Direct     (FREE!)      (With Redis
   Query                    History)
```

### Medium Query vá»›i Ollama RAG

```
1. ChromaDB semantic search â†’ Top 10 results
2. Build context from results
3. Ollama (llama3.1) generates answer
4. Return response
```

**Benefits:**

- âœ… FREE (local model)
- âœ… Fast (1-3s vs 2-5s Gemini)
- âœ… Good quality for factual queries
- âœ… No API limits

### Conversation History vá»›i Redis

```
1. User sends message â†’ Save to Redis
2. Process query
3. Get last 10 messages from Redis (< 1ms)
4. Pass to Gemini if complex
5. Save response to Redis
```

**Benefits:**

- âœ… Ultra fast (in-memory)
- âœ… Auto cleanup (TTL 7 days)
- âœ… Simple data structure
- âœ… No graph overhead

---

## ðŸ§ª Testing

### 1. Start Redis

```bash
docker-compose up -d redis
```

### 2. Test conversation vá»›i Redis

```bash
POST http://localhost:3000/chat
{
  "message": "Danh sÃ¡ch nhÃ¢n viÃªn",
  "userId": "user123"
}

# Response includes conversationId
# Stored in Redis key: conversation:CONV_xxx
```

### 3. Check Redis data

```bash
docker exec -it ekg-redis redis-cli

# List all conversation keys
KEYS conversation:*

# Get specific conversation
GET conversation:CONV_xxx

# Get user's conversations
SMEMBERS user:user123:conversations
```

### 4. Test Ollama RAG (medium query)

```bash
POST http://localhost:3000/chat
{
  "message": "TÃ¬m ngÆ°á»i giá»i Python",
  "userId": "user123"
}

# Should use Ollama RAG instead of Gemini
# Check logs for: "RAG query completed"
```

---

## ðŸ“Š Expected Impact

### Before

```
100 queries:
- Simple: 30 (Neo4j)
- Medium: 20 (ChromaDB basic)
- Complex: 50 (Gemini) â† $$$
```

### After

```
100 queries:
- Simple: 30 (Neo4j)
- Medium: 50 (Ollama RAG - FREE!) â† âœ…
- Complex: 20 (Gemini only) â† 60% cost reduction!
```

### Cost Savings

- Gemini API: ~$0.001/request
- Before: 50 calls = $0.05
- After: 20 calls = $0.02
- **Savings: 60%** + faster responses!

---

## ðŸ”§ Files Changed/Created

### New Files

1. `src/chat/services/redis-conversation.service.ts` - Redis chat storage
2. `src/chat/services/ollama-rag.service.ts` - Local LLM RAG
3. `src/ai/query-classifier.service.ts` - Improved (rewritten)
4. `docker-compose.redis.yml` - Redis setup

### Modified Files

1. `src/chat/chat.service.ts` - Redis + Ollama integration
2. `src/chat/chat.module.ts` - New providers
3. `package.json` - Added ioredis dependency

---

## âš¡ Quick Start

```bash
# 1. Install dependencies
npm install

# 2. Start Redis
docker-compose up -d redis

# 3. Start backend
npm run start:dev

# 4. Test
curl -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"TÃ¬m ngÆ°á»i giá»i AI","userId":"test"}'
```

---

## ðŸŽ¯ Next Steps (Optional Optimizations)

1. **Monitor query distribution** â†’ Adjust classifier thresholds
2. **Fine-tune Ollama prompt** â†’ Better responses
3. **Add caching layer** â†’ Redis cache for frequent queries
4. **Metrics dashboard** â†’ Track Gemini vs Ollama usage

---

**Status:** âœ… Ready to test!  
**Build:** Should pass (run `npm run build`)  
**Dependencies:** ioredis installed, Redis Docker ready
